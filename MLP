#load avalanche data
avdata0 = pd.read_csv('Jan2017_avalanchedata.csv')
avdata0.head()
date= avdata0['Date']
avdata = avdata0.drop('Date', axis=1)

below_tl = avdata['Below Treeline']
at_tl = avdata['Treeline']
above_tl = avdata['Above Treeline']
np.shape(avdata)
# averaging (31,3) to (31,1)
avg_avdata = np.mean(avdata,axis=1)
np.shape(avg_avdata)

#Visualize avalanche data
plt.figure(figsize=(10,4))
plt.subplot(1,2,1)
plt.plot(date, below_tl, color = 'c',label='Below Treeline')
plt.plot(date, at_tl, color = 'g')
plt.plot(date, above_tl, color = 'r', linestyle='dashed')
plt.xlabel('Days of January')
plt.xticks(fontsize='0')
plt.ylabel('Risk Level')
plt.legend(['Below Treeline', 'Treeline', 'Above Treeline'] )
plt.title('Avalanche Risk for 3 Zones')
plt.subplot(1,2,2)
plt.plot(date, avg_avdata, color = 'k',label = 'mean',marker = 'o', linewidth = 2)
plt.xlabel('Days of January')
plt.xticks(fontsize='0')
plt.ylabel('Risk Level')
plt.title('Average Avalanche Risk')
plt.show()

#turn into arrays (could do this at start if you want)
Array2d_avdata = avdata.to_numpy()
print(Array2d_avdata)
Array2d_avg_avdata = avg_avdata.to_numpy()
print(Array2d_avg_avdata)

#reshape avalanche data
avg_avdata=Array2d_avg_avdata.reshape(-1, 1)
np.shape(avg_avdata)

#target variable: y (avalanche risk)
y = avg_avdata
y-=np.min(avg_avdata)
y/=np.max(avg_avdata) #now y ranges from 0 to 1
#y

#predictor variable(s): x (Temp and Snow Depth)
x = ([temp_norm, snowdep_norm])
np.shape(x) ##this is an issue because shape is (2,31,1). Maybe if we dont reshape originally we can get it into a (31x2) matrix?

ntrain = 23 #23 is about 75% of 31
x_train = PCs[:ntrain,:1] #train on 23 observations of first 1 PC. IS THERE A WAY TO DO THIS WITHOUT PCs?
y_train = y[:ntrain]
x_test = PCs[ntrain:,:1] #test on remaining observations
y_test = y[ntrain:]

##Code from LAB 9 below##

num_models = 10 #number of models to build for the ensemble
min_nhn = 1 #minimum number of hidden neurons to loop through (nhn = 'number hidden neurons')
max_nhn = 9 #maximum number of hidden neurons to loop through
max_hidden_layers = 1 #maximum number of hidden layers to loop through (nhl = 'number hidden layers')
batch_size = 32
solver = 'adam' #use stochastic gradient descent as an optimization method (weight updating algorithm)
activation = 'relu'
learning_rate_init = 0.01
###

max_iter = 1500 #max number of epochs to run
early_stopping = True #True = stop early if validation error begins to rise
validation_fraction = 0.1 #fraction of training data to use as validation

y_out_all_nhn = []
y_out_ensemble = []
RMSE_ensemble = [] #RMSE for each model in the ensemble
RMSE_ensemble_cumsum = [] #RMSE of the cumulative saltation for each model
nhn_best = []
nhl_best = []

for model_num in range(num_models): #for each model in the ensemble
    
    print('Model Number: ' + str(model_num))
    
    RMSE = []
    y_out_all_nhn = []
    nhn = []
    nhl = []
    
    for num_hidden_layers in range(1,max_hidden_layers+1):
    
        print('\t # Hidden Layers = ' + str(num_hidden_layers))
    
        for num_hidden_neurons in range(min_nhn,max_nhn+1): #for each number of hidden neurons

            print('\t\t # hidden neurons = ' + str(num_hidden_neurons))
            
            hidden_layer_sizes = (num_hidden_neurons,num_hidden_layers)
            model = MLPRegressor(hidden_layer_sizes=hidden_layer_sizes, 
                                 verbose=False,
                                 max_iter=max_iter, 
                                 early_stopping = early_stopping,
                                 validation_fraction = validation_fraction,
                                 batch_size = batch_size,
                                 solver = solver,
                                 activation = activation,
                                 learning_rate_init = learning_rate_init)

            model.fit(x_train,y_train) #train the model

            y_out_this_nhn = model.predict(x_test) #model prediction for this number of hidden neurons (nhn)
            y_out_all_nhn.append(y_out_this_nhn) #store all models -- will select best one best on RMSE

            RMSE.append(rmse(y_test,y_out_this_nhn)) #RMSE between cumulative curves
            
            nhn.append(num_hidden_neurons)
            nhl.append(num_hidden_layers)
        
    indBest = RMSE.index(np.min(RMSE)) #index of model with lowest RMSE
    RMSE_ensemble.append(np.min(RMSE))
    nhn_best.append(nhn[indBest])
    nhl_best.append(nhl[indBest])
    #nhn_best.append(indBest+1) #the number of hidden neurons that achieved best model performance of this model iteration
    y_out_ensemble.append(y_out_all_nhn[indBest])
    
    print('\t BEST: ' + str(nhl_best[model_num]) + ' hidden layers, '+ str(nhn_best[model_num]) + ' hidden neurons')
    
y_out_ensemble_mean = np.mean(y_out_ensemble,axis=0)
RMSE_ensemble_mean = rmse(y_out_ensemble_mean,y_test)

#visualize
plt.figure(figsize=(12,8))
plt.subplot(241)
plt.scatter(len(RMSE_ensemble),RMSE_ensemble_mean,c='k',marker='*')
plt.scatter(range(len(RMSE_ensemble)),RMSE_ensemble)
plt.xlabel('Model #')
plt.ylabel('RMSE')
plt.title('Error')
plt.subplot(242)
plt.scatter(range(len(nhn_best)),nhn_best)
plt.xlabel('Model #')
plt.ylabel('# Hidden Neurons')
plt.title('Hidden Neurons')
plt.subplot(243)
plt.scatter(range(len(nhl_best)),nhl_best)
plt.xlabel('Model #')
plt.ylabel('# Hidden Layers')
plt.title('Hidden Layers')
plt.subplot(244)
plt.scatter(y_test,y_out_ensemble_mean)
#plt.plot((np.min(y_test),np.max(y_test)),'k--')
plt.xlabel('y_test')
plt.ylabel('y_model')
plt.title('Ensemble')
plt.subplot(212)
plt.plot(y_out_ensemble_mean)
plt.plot(np.array(y_test),alpha = 0.5)
plt.tight_layout()

#visualize individual model runs
saveIt = 0
plt.figure(figsize = (12,5))
plt.scatter(range(len(y_test)),y_test,label='Observations',zorder = 0,alpha = 0.3)
plt.plot(range(len(y_test)),np.transpose(y_out_ensemble[0]),color = 'k',alpha = 0.4,label='Individual Models',zorder=1) #plot first ensemble member with a label
plt.plot(range(len(y_test)),np.transpose(y_out_ensemble[1:]),color = 'k',alpha = 0.4,zorder=1) #plot remaining ensemble members without labels for a nicer legend
plt.plot(range(len(y_test)),y_out_ensemble_mean,color = 'k',label = 'Ensemble',zorder=2, linewidth = 3)
plt.xlabel('Time', fontsize = 20)
plt.ylabel('y', fontsize = 20)
plt.xticks(fontsize = 16)
plt.yticks(fontsize = 16)
plt.title('MLP Model Results', fontsize = 24)
plt.legend(fontsize = 16, loc = 'best')
plt.tight_layout()
if saveIt:
    plt.savefig('tutorial10_fig12.png')
plt.show()

#visualize performance metrics/etc
saveIt = 0
plt.figure(figsize=(16,4))
plt.subplot(131)
plt.scatter(len(RMSE_ensemble),RMSE_ensemble_mean,c='k',marker='*', s = 150)
plt.scatter(range(len(RMSE_ensemble)),RMSE_ensemble, s = 150)
plt.xlabel('Model #', fontsize = 20)
plt.ylabel('RMSE', fontsize = 20)
plt.xticks(fontsize = 16)
plt.yticks(fontsize = 16)
#plt.ylim((np.min(RMSE_ensemble) - 0.005, np.max(RMSE_ensemble)+0.005))
plt.title('Error', fontsize = 20)
plt.subplot(132)
plt.scatter(range(len(nhn_best)),nhn_best, s = 150)
plt.xlabel('Model #', fontsize = 20)
plt.ylabel('# Hidden Neurons', fontsize = 20)
plt.xticks(fontsize = 16)
plt.yticks(fontsize = 16)
plt.title('Hidden Neurons', fontsize = 20)
plt.subplot(133)
plt.scatter(range(len(nhl_best)),nhl_best, s = 150)
plt.xlabel('Model #', fontsize = 20)
plt.ylabel('# Hidden Layers', fontsize = 20)
plt.xticks(fontsize = 16)
plt.yticks(fontsize = 16)
plt.title('Hidden Layers', fontsize = 20)
plt.tight_layout()
if saveIt:
    plt.savefig('tutorial10_fig10.png')
plt.show()

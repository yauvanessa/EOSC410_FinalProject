{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "# from functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define RMSE as a function, since we'll use this in the NN model \n",
    "def rmse(target,prediction):\n",
    "    return(np.sqrt(((target - prediction)**2).sum()/len(target)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "AxisError",
     "evalue": "axis 2 is out of bounds for array of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAxisError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/Vanessa/Desktop/EOSC410/Project/project.ipynb Cell 3\u001b[0m in \u001b[0;36m<cell line: 29>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/Vanessa/Desktop/EOSC410/Project/project.ipynb#W2sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m snowdep \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mappend(snowdep_J2017,snowdep_F2017)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/Vanessa/Desktop/EOSC410/Project/project.ipynb#W2sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39m# import era5 coordinates\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/Vanessa/Desktop/EOSC410/Project/project.ipynb#W2sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39m# coord = np.load('Sea-to-Sky_JanFeb2017_LatLon.npy', allow_pickle=True)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/Vanessa/Desktop/EOSC410/Project/project.ipynb#W2sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39m# lat = coord[0]\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/Vanessa/Desktop/EOSC410/Project/project.ipynb#W2sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/Vanessa/Desktop/EOSC410/Project/project.ipynb#W2sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39m# averaging TEMP from (31,7,13) to (31, 7)\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/Vanessa/Desktop/EOSC410/Project/project.ipynb#W2sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m temp_station_mean \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mmean(temp_2m,axis\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/Vanessa/Desktop/EOSC410/Project/project.ipynb#W2sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39m# averaging TEMP from (31,7) to (31,1)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/Vanessa/Desktop/EOSC410/Project/project.ipynb#W2sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m temp_mean \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmean(temp_station_mean,axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m<__array_function__ internals>:5\u001b[0m, in \u001b[0;36mmean\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/env_eosc410/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3440\u001b[0m, in \u001b[0;36mmean\u001b[0;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[1;32m   3437\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   3438\u001b[0m         \u001b[39mreturn\u001b[39;00m mean(axis\u001b[39m=\u001b[39maxis, dtype\u001b[39m=\u001b[39mdtype, out\u001b[39m=\u001b[39mout, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m-> 3440\u001b[0m \u001b[39mreturn\u001b[39;00m _methods\u001b[39m.\u001b[39;49m_mean(a, axis\u001b[39m=\u001b[39;49maxis, dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[1;32m   3441\u001b[0m                       out\u001b[39m=\u001b[39;49mout, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/env_eosc410/lib/python3.9/site-packages/numpy/core/_methods.py:167\u001b[0m, in \u001b[0;36m_mean\u001b[0;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[1;32m    163\u001b[0m arr \u001b[39m=\u001b[39m asanyarray(a)\n\u001b[1;32m    165\u001b[0m is_float16_result \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m--> 167\u001b[0m rcount \u001b[39m=\u001b[39m _count_reduce_items(arr, axis, keepdims\u001b[39m=\u001b[39;49mkeepdims, where\u001b[39m=\u001b[39;49mwhere)\n\u001b[1;32m    168\u001b[0m \u001b[39mif\u001b[39;00m rcount \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39mif\u001b[39;00m where \u001b[39mis\u001b[39;00m \u001b[39mTrue\u001b[39;00m \u001b[39melse\u001b[39;00m umr_any(rcount \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m, axis\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    169\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\u001b[39m\"\u001b[39m\u001b[39mMean of empty slice.\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mRuntimeWarning\u001b[39;00m, stacklevel\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/env_eosc410/lib/python3.9/site-packages/numpy/core/_methods.py:76\u001b[0m, in \u001b[0;36m_count_reduce_items\u001b[0;34m(arr, axis, keepdims, where)\u001b[0m\n\u001b[1;32m     74\u001b[0m     items \u001b[39m=\u001b[39m nt\u001b[39m.\u001b[39mintp(\u001b[39m1\u001b[39m)\n\u001b[1;32m     75\u001b[0m     \u001b[39mfor\u001b[39;00m ax \u001b[39min\u001b[39;00m axis:\n\u001b[0;32m---> 76\u001b[0m         items \u001b[39m*\u001b[39m\u001b[39m=\u001b[39m arr\u001b[39m.\u001b[39mshape[mu\u001b[39m.\u001b[39;49mnormalize_axis_index(ax, arr\u001b[39m.\u001b[39;49mndim)]\n\u001b[1;32m     77\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     \u001b[39m# TODO: Optimize case when `where` is broadcast along a non-reduction\u001b[39;00m\n\u001b[1;32m     79\u001b[0m     \u001b[39m# axis and full sum is more excessive than needed.\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \n\u001b[1;32m     81\u001b[0m     \u001b[39m# guarded to protect circular imports\u001b[39;00m\n\u001b[1;32m     82\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mnumpy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mstride_tricks\u001b[39;00m \u001b[39mimport\u001b[39;00m broadcast_to\n",
      "\u001b[0;31mAxisError\u001b[0m: axis 2 is out of bounds for array of dimension 1"
     ]
    }
   ],
   "source": [
    "## EOSC410 Project\n",
    "## Authors: Vanessa Yau + Christina Rutherford\n",
    "saveIt = 0 # set to 1 to save all figs\n",
    "\n",
    "# import avalanche data for sea-to-sky region\n",
    "avy_risk = pd.read_csv('sea-to-sky_JanFeb2017.csv') #TODO do we need to normalize this data?\n",
    "avy_date = avy_risk['Date']\n",
    "below_tree = avy_risk['Below Treeline']; below_tree = np.array(below_tree)\n",
    "tree = avy_risk['Treeline']; tree = np.array(tree)\n",
    "above_tree = avy_risk['Above Treeline']; above_tree = np.array(above_tree)\n",
    "\n",
    "# import era5 2m temp data\n",
    "temp_2m_J2017 = np.load('Sea-to-Sky_Jan2017_Temp2m.npy')\n",
    "temp_2m_F2017 = np.load('Sea-to-Sky_Feb2017_Temp2m.npy')\n",
    "temp_2m = np.append(temp_2m_J2017,temp_2m_F2017)\n",
    "snowdep_J2017 = np.load('Sea-to-Sky_Jan2017_SnowDepth.npy')\n",
    "snowdep_F2017 = np.load('Sea-to-Sky_Feb2017_SnowDepth.npy')\n",
    "snowdep = np.append(snowdep_J2017,snowdep_F2017)\n",
    "\n",
    "# import era5 coordinates\n",
    "# coord = np.load('Sea-to-Sky_JanFeb2017_LatLon.npy', allow_pickle=True)\n",
    "# lat = coord[0]\n",
    "# lon = coord[1]\n",
    "\n",
    "#Current Temp and Snow Depth data are (31, 7, 13). I averaged them below to be (31,) and eventually reshape them to (31,1), but this still \n",
    "#causes the variance to only be 1 mode in the center. Therefore maybe we don't do PCA \n",
    "\n",
    "# averaging TEMP from (31,7,13) to (31, 7)\n",
    "temp_station_mean = np.mean(temp_2m,axis=2)\n",
    "# averaging TEMP from (31,7) to (31,1)\n",
    "temp_mean = np.mean(temp_station_mean,axis=1)\n",
    "# averaging SNOW from (31, 7, 13) to (31, 7)\n",
    "snowdep_station_mean = np.mean(snowdep,axis=2)\n",
    "# averaging SNOW from (31, 7) to (31, 1)\n",
    "snowdep_mean = np.mean(snowdep_station_mean,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize current temp and snow depth data\n",
    "plt.figure(figsize=(10,4))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(temp_2m[:,1])\n",
    "plt.plot(temp_mean, color = 'k',label = 'mean', linewidth = 3)\n",
    "plt.xlabel('Days from Jan 1')\n",
    "plt.ylabel('Temp (K)')\n",
    "plt.title('Daily Temperature at noon, Jan and Feb 2017')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(snowdep[:,1])\n",
    "plt.plot(snowdep_mean, color = 'k',label = 'mean', linewidth = 3)\n",
    "plt.xlabel('Days from Jan 1')\n",
    "plt.ylabel('Snow Depth (m of water equivalent)')\n",
    "plt.title('Daily Snow Depth at noon, Jan and Feb 2017')\n",
    "\n",
    "plt.tight_layout()\n",
    "# plt.show()\n",
    "if saveIt:\n",
    "    plt.savefig('fig_JanFeb2017_temp_snowdep.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize data\n",
    "temp_norm = (temp_mean - temp_mean.mean())/temp_mean.std()\n",
    "snowdep_norm = (snowdep_mean - snowdep_mean.mean())/snowdep_mean.std()\n",
    "# reshape data from (31,) to (31, 1)\n",
    "temp_norm=temp_norm.reshape(-1, 1)\n",
    "snowdep_norm=snowdep_norm.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize normalized data\n",
    "plt.figure(figsize=(10,4))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(temp_norm)\n",
    "#plt.plot(temp_mean, color = 'k',label = 'mean', linewidth = 3)\n",
    "plt.xlabel('Day from Jan 1')\n",
    "plt.ylabel('Temp (K)')\n",
    "plt.title('Normalized Temp January and February 2017')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(snowdep_norm)\n",
    "#plt.plot(snowdep_mean, color = 'k',label = 'mean', linewidth = 3)\n",
    "plt.xlabel('Day from Jan 1')\n",
    "plt.ylabel('Snow Depth (m of water equivalent)')\n",
    "plt.title('Normalized Snow Depth January and February 2017')\n",
    "\n",
    "plt.tight_layout()\n",
    "# plt.show()\n",
    "if saveIt == 1:\n",
    "    plt.savefig('fig_JanFeb2017_normalized_temp_snowdep.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot avy risk data\n",
    "days = np.linspace(1,31,31)\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.scatter(days,above_tree,color='blue',label='Above Treeline',alpha=0.5)\n",
    "plt.scatter(days,tree,color='green',label='Treeline',alpha=0.5)\n",
    "plt.scatter(days,below_tree,color='orange',label='Below Treeline',alpha=0.5)\n",
    "plt.legend()\n",
    "plt.xlabel('Days from Jan 1')\n",
    "plt.ylabel('Avalanche Risk Index Value')\n",
    "plt.title('Avalanche Risk By Terrain in January and February 2017')\n",
    "\n",
    "plt.tight_layout()\n",
    "if saveIt == 1:\n",
    "    plt.savefig('fig_JanFeb2017_avyrisk.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn data into one hot vectors for categorical model\n",
    "# below treeline\n",
    "enc_bt = OneHotEncoder(max_categories=5).fit(below_tree.values.reshape(-1,1))\n",
    "enc_bt.categories_\n",
    "bt_cat = enc_bt.transform(below_tree.values.reshape(-1,1)).toarray()\n",
    "\n",
    "# if len(bt_cat[0,:]) != 5: # checks that output has 5 categories, otherwise adds colums of zeros\n",
    "#     start = len(bt_cat[0,:])\n",
    "#     end = 5\n",
    "#     diff = end-start\n",
    "#     addto = len(bt_cat[:,0])\n",
    "#     for c in range(0,diff):\n",
    "#         bt_cat = np.c_[bt_cat, np.zeros(addto)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ready for MLP, see MLP file\n",
    "# define variables for MLP\n",
    "N = len(days) # size of data\n",
    "n_predictors = 2\n",
    "predictors = np.c_[temp_norm,snowdep_norm]\n",
    "\n",
    "target_bt = bt_cat\n",
    "target_t = tree\n",
    "target_at = above_tree\n",
    "target = [target_bt, target_t, target_at]\n",
    "\n",
    "# loop through the 3 terrain categories for MLP on each\n",
    "fracTrain = 0.8 # 80% of data used for training\n",
    "NTrain = int(np.floor(fracTrain*N))\n",
    "for m in range(0,2): \n",
    "    # target_m = target[m]\n",
    "    target_m = target_bt\n",
    "    x_train = predictors[:NTrain]\n",
    "    y_train = target_m[:NTrain]\n",
    "\n",
    "    x_test = predictors[NTrain:]\n",
    "    y_test = target_m[NTrain:]\n",
    "\n",
    "    # y_out_ensemble_mean, y_out_ensemble, RMSE_ensemble_mean, RMSE_ensemble, nhn_best, nhl_best = MLP(x_train,y_train,x_test,y_test)\n",
    "\n",
    "    num_models = 10 #number of models to build for the ensemble\n",
    "    min_nhn = 1 #minimum number of hidden neurons to loop through (nhn = 'number hidden neurons')\n",
    "    max_nhn = 8 #maximum number of hidden neurons to loop through\n",
    "    max_hidden_layers = 1 #maximum number of hidden layers to loop through (nhl = 'number hidden layers')\n",
    "    batch_size = 32\n",
    "    solver = 'adam' #use stochastic gradient descent as an optimization method (weight updating algorithm)\n",
    "    activation = 'relu' # TODO try 'softmax' since we are dealing with multiclasses... changed from 'relu'\n",
    "    learning_rate_init = 0.001\n",
    "\n",
    "    max_iter = 1500 #max number of epochs to run\n",
    "    early_stopping = True #True = stop early if validation error begins to rise\n",
    "    validation_fraction = 0.1 #fraction of training data to use as validation\n",
    "\n",
    "    y_out_all_nhn = []\n",
    "    y_out_ensemble = []\n",
    "    RMSE_ensemble = [] #RMSE for each model in the ensemble\n",
    "    RMSE_ensemble_cumsum = [] #RMSE of the cumulative saltation for each model\n",
    "    nhn_best = []\n",
    "    nhl_best = []\n",
    "\n",
    "    for model_num in range(num_models): #for each model in the ensemble\n",
    "        \n",
    "        # print('Model Number: ' + str(model_num))\n",
    "        \n",
    "        RMSE = []\n",
    "        y_out_all_nhn = []\n",
    "        nhn = []\n",
    "        nhl = []\n",
    "        \n",
    "        for num_hidden_layers in range(1,max_hidden_layers+1):\n",
    "        \n",
    "            # print('\\t # Hidden Layers = ' + str(num_hidden_layers))\n",
    "        \n",
    "            for num_hidden_neurons in range(min_nhn,max_nhn+1): #for each number of hidden neurons\n",
    "\n",
    "                # print('\\t\\t # hidden neurons = ' + str(num_hidden_neurons))\n",
    "                \n",
    "                hidden_layer_sizes = (num_hidden_neurons,num_hidden_layers)\n",
    "                model = MLPRegressor(hidden_layer_sizes=hidden_layer_sizes, \n",
    "                                    verbose=False,\n",
    "                                    max_iter=max_iter, \n",
    "                                    early_stopping = early_stopping,\n",
    "                                    validation_fraction = validation_fraction,\n",
    "                                    batch_size = batch_size,\n",
    "                                    solver = solver,\n",
    "                                    activation = activation,\n",
    "                                    learning_rate_init = learning_rate_init)\n",
    "\n",
    "                model.fit(x_train,y_train) #train the model\n",
    "\n",
    "                y_out_this_nhn = model.predict(x_test) #model prediction for this number of hidden neurons (nhn)\n",
    "                y_out_all_nhn.append(y_out_this_nhn) #store all models -- will select best one best on RMSE\n",
    "\n",
    "                RMSE.append(rmse(y_test,y_out_this_nhn)) #RMSE between cumulative curves\n",
    "                \n",
    "                nhn.append(num_hidden_neurons)\n",
    "                nhl.append(num_hidden_layers)\n",
    "            \n",
    "        indBest = RMSE.index(np.min(RMSE)) #index of model with lowest RMSE\n",
    "        RMSE_ensemble.append(np.min(RMSE))\n",
    "        nhn_best.append(nhn[indBest])\n",
    "        nhl_best.append(nhl[indBest])\n",
    "        #nhn_best.append(indBest+1) #the number of hidden neurons that achieved best model performance of this model iteration\n",
    "        y_out_ensemble.append(y_out_all_nhn[indBest])\n",
    "        \n",
    "        # print('\\t BEST: ' + str(nhl_best[model_num]) + ' hidden layers, '+ str(nhn_best[model_num]) + ' hidden neurons')\n",
    "        \n",
    "    y_out_ensemble_mean = np.mean(y_out_ensemble,axis=0)\n",
    "    RMSE_ensemble_mean = rmse(y_out_ensemble_mean,y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform y output back to categorical 1D array\n",
    "bt_model = enc_bt.inverse_transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot models only\n",
    "plt.scatter(days[NTrain:],below_tree[NTrain:],alpha=0.8,marker='*',color='b')\n",
    "plt.scatter(days[NTrain:],bt_model,alpha=0.8,color='b')\n",
    "plt.xlabel('Day', fontsize = 20)\n",
    "plt.ylabel('Avalanche Danger Rating', fontsize = 20)\n",
    "plt.xticks(fontsize = 16)\n",
    "plt.yticks(fontsize = 16)\n",
    "plt.title('MLP Model Results', fontsize = 24)\n",
    "plt.legend(fontsize = 16, loc = 'best')\n",
    "\n",
    "plt.tight_layout()\n",
    "if saveIt:\n",
    "    plt.savefig(f'fig_Jan2017_model_results_{m}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.subplot(241)\n",
    "plt.scatter(len(RMSE_ensemble),RMSE_ensemble_mean,c='k',marker='*')\n",
    "plt.scatter(range(len(RMSE_ensemble)),RMSE_ensemble)\n",
    "plt.xlabel('Model #')\n",
    "plt.ylabel('RMSE')\n",
    "plt.title('Error')\n",
    "\n",
    "plt.subplot(242)\n",
    "plt.scatter(range(len(nhn_best)),nhn_best)\n",
    "plt.xlabel('Model #')\n",
    "plt.ylabel('# Hidden Neurons')\n",
    "plt.title('Hidden Neurons')\n",
    "\n",
    "plt.subplot(243)\n",
    "plt.scatter(range(len(nhl_best)),nhl_best)\n",
    "plt.xlabel('Model #')\n",
    "plt.ylabel('# Hidden Layers')\n",
    "plt.title('Hidden Layers')\n",
    "\n",
    "plt.subplot(244)\n",
    "plt.scatter(y_test,y_out_ensemble_mean)\n",
    "#plt.plot((np.min(y_test),np.max(y_test)),'k--')\n",
    "plt.xlabel('y_test')\n",
    "plt.ylabel('y_model')\n",
    "plt.title('Ensemble')\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.scatter(days[NTrain:],below_tree[NTrain:],alpha=0.8,marker='*',color='b')\n",
    "plt.scatter(days[NTrain:],bt_model,alpha=0.8,color='b')\n",
    "plt.xlabel('Day', fontsize = 20)\n",
    "plt.ylabel('Avalanche Danger Rating', fontsize = 20)\n",
    "plt.xticks(fontsize = 16)\n",
    "plt.yticks(fontsize = 16)\n",
    "plt.title('MLP Model Results', fontsize = 24)\n",
    "plt.legend(fontsize = 16, loc = 'best')\n",
    "\n",
    "plt.tight_layout()\n",
    "if saveIt:\n",
    "    plt.savefig(f'fig_Jan2017_model_results_{m}.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('env_eosc410')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dbcbb06156fac4739d2dc103eb6256571b7997e7c67fce99949d7399dc5decb0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
